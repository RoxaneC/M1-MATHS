{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkcyan>  Expectation Maximization for latent data models </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case where we are interested in estimating unknown parameters $\\theta\\in\\mathbb{R}^m$ characterizing a model with missing data, the Expectation Maximization (EM) algorithm (Dempster et al. 1977) can be used when the joint distribution of the missing data $X$ and the observed data $Y$ is explicit. For all $\\theta\\in\\mathbb{R}^m$, let $p_{\\theta}$ be the probability density function of $(X,Y)$ when the model is parameterized by $\\theta$ with respect to a given reference measure $\\mu$. The EM algorithm aims at computing iteratively an approximation of the maximum likelihood estimator which maximizes the observed data loglikelihood:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta;Y) = \\log p_{\\theta}(Y) =\\log \\int p_{\\theta}(x,Y)\\mu(\\mathrm{d}x)\\,.\n",
    "$$\n",
    "\n",
    "As this quantity cannot be computed explicitly in general cases, the EM algorithm finds the maximum likelihood estimator by iteratively maximizing the expected complete data loglikelihood.\n",
    " \n",
    "Start with an inital value $\\theta^{(0)}$ and let $\\theta^{(t)}$ be the estimate at the $t$-th iteration for $t\\geqslant 0$, then the next iteration of EM is decomposed into two steps.\n",
    "\n",
    "1. E-step. Compute the expectation of the complete data loglikelihood, with respect to the conditional distribution of the missing data given the observed data parameterized by $\\theta^{(t)}$:\n",
    "\n",
    "$$\n",
    "Q(\\theta,\\theta^{(t)}) =\\mathbb{E}_{\\theta^{(t)}}\\left[\\log p_{\\theta}(X,Y)|Y \\right]\\,.\n",
    "$$\n",
    "\n",
    "2. M step. Determine $\\theta^{(t+1)}$ by maximizing the function Q:\n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)}\\in \\mbox{argmax}_\\theta Q(\\theta,\\theta^{(t)})\\,.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 \n",
    "Prove the following crucial property motivates the EM algorithm.  \n",
    "For all $\\theta,\\theta^{(t)}$,\n",
    "    \n",
    "$$\n",
    "\\ell(Y;\\theta) - \\ell(Y;\\theta^{(t)}) \\geqslant Q(\\theta,\\theta^{(t)})-Q(\\theta^{(t)},\\theta^{(t)})\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, $X = (X_1,\\ldots,X_n)$ and $Y = (Y_1,\\ldots,Y_n)$ where $\\{(X_i,Y_i)\\}_{1\\leqslant i\\leqslant n}$  are i.i.d. in $\\{-1,1\\} \\times \\mathbb{R}^d$. For $k\\in\\{-1,1\\}$, write $\\pi_k = \\mathbb{P}(X_1 = k)$. Assume that, conditionally on the event $\\{X_1 = k\\}$, $Y_1$ has a Gaussian distribution with mean $\\mu_k \\in\\mathbb{R}^d$ and covariance matrix $\\Sigma\\in \\mathbb{R}^{d\\times d}$. In this case, the parameter $\\theta=(\\pi_1, \\mu_1,\\mu_{-1}, \\Sigma)$ belongs to the set $\\Theta= [0,1] \\times \\mathbb{R}^d \\times \\mathbb{R}^d \\times \\mathbb{R}^{d \\times d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "- Write the complete data loglikelihood\n",
    "- Let $\\theta^{(t)}$ be the current parameter estimate. Compute $\\theta\\mapsto Q(\\theta,\\theta^{(t)})$.\n",
    "- Compute $\\theta^{(t+1)} = \\mathrm{argmax}_\\theta Q(\\theta,\\theta^{(t)})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=darkorange>  Simulated data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "- Sample data from a mixture of two Gaussian distributions and display the associated histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.96297096  4.08794428 -0.83680086  2.01867067  2.16030249  0.3305739\n",
      "  2.68739799  3.52991335  3.58736134  3.6080082   4.20392076  3.80755604\n",
      "  3.10589452  2.1846676   3.0184785   3.61663413  3.40423778 -1.81028447\n",
      "  2.50291769  3.51519087 -0.87328315  3.03969585  3.15196953  5.01981048\n",
      "  3.6201004  -3.57648165 -1.9247449   3.28314678  3.26444986  2.56805003\n",
      "  2.39186825  3.0336467   2.19938613  3.15448247  3.67710445 -3.94449185\n",
      "  1.46234122  3.6547749  -1.26291492 -2.40492904  2.44286515  3.59310587\n",
      "  4.18329752  2.22334587 -1.10523642 -3.16922117  3.02098967 -2.60632901\n",
      "  3.83850345 -3.08593934  0.29886659 -0.16283934  4.28230297  3.51997652\n",
      "  3.42533177  3.07127598  2.70219308  4.33746412  3.06969895  3.90337778\n",
      "  2.96682897  3.04274847  2.24360057  3.38057623  5.94784506 -2.63870111\n",
      " -2.5436698  -0.72483185  2.45874423  3.53121408 -0.8359017   2.04249367\n",
      "  2.31328889 -1.99747163  3.80275794  3.6256046   2.8504212   3.32197956\n",
      " -1.42600798 -2.25333427  2.84987814  4.13453226  3.40660227  4.17008334\n",
      "  2.77172953  2.40853522  0.13124729 -1.96727331 -2.47332003 -1.99278855\n",
      "  1.34009567  3.00301111  4.21997158  1.39573905  2.06521902 -1.17189761\n",
      "  3.00566731  3.58307265  5.14321146 -0.5690262 ]\n"
     ]
    }
   ],
   "source": [
    "# number of observations\n",
    "n_samples = 100\n",
    "\n",
    "# means and variance to be estimated\n",
    "mu1, sigma1 = -2, 1.5\n",
    "mu2, sigma2 = 3, 1\n",
    "mu = np.array([mu1, mu2])\n",
    "sigma = np.array([sigma1, sigma2])\n",
    "\n",
    "# prior probability of the first cluster/goup \n",
    "pi1 = 0.3\n",
    "\n",
    "# A compléter\n",
    "X = np.random.normal(0, 1, n_samples)\n",
    "loi = np.random.choice(2, size=n_samples, p=[pi1, 1-pi1])\n",
    "mix_gauss = mu[loi] + np.sqrt(sigma[loi])*X\n",
    "\n",
    "print(mix_gauss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian_pdf(x, mean, variance):\n",
    "  z = np.exp(-(x - mean)**2/(2*variance))/np.sqrt(2*np.pi*variance)\n",
    "  return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the training data\n",
    "\n",
    "# A compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=darkorange>  EM algorithm</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot estimated density - plot the density of a mixture of two Gaussian distributions\n",
    "\n",
    "# A compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    " - Compute $\\omega_t^i = \\mathbb{P}_{\\theta^{(t)}}(X_i=1|Y_i)$.\n",
    " - Write the loop of the EM algorithm.\n",
    " - Run the algorithm and display the loglikelihood and the estimates along iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clust = 2\n",
    "weights = np.ones((n_clust)) / n_clust\n",
    "means = [0, 1]\n",
    "variances = [0.5, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of iterations of the EM algorithm\n",
    "n_it = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi1_est = np.zeros(n_it+1)\n",
    "mu1_est = np.zeros(n_it+1)\n",
    "mu2_est = np.zeros(n_it+1)\n",
    "sigma1_est = np.zeros(n_it+1)\n",
    "sigma2_est = np.zeros(n_it+1)\n",
    "\n",
    "pi1_est[0] = weights[0]\n",
    "mu1_est[0] = means[0]\n",
    "mu2_est[0] = means[1]\n",
    "sigma1_est[0] = variances[0]\n",
    "sigma2_est[0] = variances[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A compléter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loglikelihood along iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot parameters along iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "\n",
    "- Explore the sensitivity of the algorithm with respect to i) the initial estimates, ii) the number of iterations, iii) the number of observations and iv) the number of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A compléter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
